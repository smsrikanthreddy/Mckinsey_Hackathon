{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train file:- (69713, 22)\n",
      "shape of test file:- (30037, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "DATAPATH = 'F:/srikanth/data/k_data/AV/Mckinsay_Hackathon'\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATAPATH,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATAPATH,'test.csv'))\n",
    "print('shape of train file:-', np.shape(train))\n",
    "print('shape of test file:-', np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Monthly_Income', 'Existing_EMI', 'Loan_Amount', 'Loan_Period',\n",
      "       'Interest_Rate', 'EMI', 'Var1', 'noofDays', 'dob_day', 'dob_dayofweek',\n",
      "       ...\n",
      "       'Source_S159', 'Source_S161', 'Source_Category_A', 'Source_Category_B',\n",
      "       'Source_Category_C', 'Source_Category_D', 'Source_Category_E',\n",
      "       'Source_Category_F', 'Source_Category_G', 'Approved'],\n",
      "      dtype='object', length=237)\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv(os.path.join(DATAPATH,\"train_preprocessed.csv\"))\n",
    "#test = pd.read_csv(os.path.join(DATAPATH,\"test_preprocessed.csv\"))\n",
    "#labels = pd.read_csv(os.path.join(DATAPATH,\"train_labels.csv\"), header = None)\n",
    "#test_ids = pd.read_csv(os.path.join(DATAPATH,\"test_ids.csv\"), header = None)\n",
    "#labels = list(labels.iloc[:,0])\n",
    "#train['Approved'] = labels\n",
    "#train.to_csv(os.path.join(DATAPATH,\"train_preprocessed_ftrl.csv\"), index = False) #for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train= os.path.join(DATAPATH,\"train_preprocessed_ftrl.csv\")\n",
    "test=os.path.join(DATAPATH,\"test_preprocessed.csv\")\n",
    "submission = os.path.join(DATAPATH,\"results\",\"ftrl_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "from random import random\n",
    "import pickle\n",
    "\n",
    "#configuring model parameters\n",
    "alpha = .05 \n",
    "beta = 1. \n",
    "L1 = 0.\n",
    "L2 = 1.\n",
    "\n",
    "# C, feature/hash trick\n",
    "D = 2 ** 24            \n",
    "interaction = False \n",
    "# D, training/validation\n",
    "epoch = 4       # learn training data for N passes\n",
    "holdafter = 9   # data after date N (exclusive) are used as validation\n",
    "holdout = 200  # use every N training instance for holdout validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ftrl_proximal(object):\n",
    "    ''' Our main algorithm: Follow the regularized leader - proximal\n",
    "        In short,\n",
    "        this is an adaptive-learning-rate sparse logistic-regression with\n",
    "        efficient L1-L2-regularization\n",
    "        Reference:\n",
    "        http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, alpha, beta, L1, L2, D, interaction):\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "\n",
    "        # feature related parameters\n",
    "        self.D = D\n",
    "        self.interaction = interaction\n",
    "\n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        self.n = [0.] * D\n",
    "        self.z = [random() for k in range(D)]#[0.] * D\n",
    "        self.w = {}\n",
    "\n",
    "    def _indices(self, x):\n",
    "        ''' A helper generator that yields the indices in x\n",
    "            The purpose of this generator is to make the following\n",
    "            code a bit cleaner when doing feature interaction.\n",
    "        '''\n",
    "\n",
    "        # first yield index of the bias term\n",
    "        yield 0\n",
    "\n",
    "        # then yield the normal indices\n",
    "        for index in x:\n",
    "            yield index\n",
    "\n",
    "        # now yield interactions (if applicable)\n",
    "        if self.interaction:\n",
    "            D = self.D\n",
    "            L = len(x)\n",
    "\n",
    "            x = sorted(x)\n",
    "            for i in xrange(L):\n",
    "                for j in xrange(i+1, L):\n",
    "                    # one-hot encode interactions with hash trick\n",
    "                    yield abs(hash(str(x[i]) + '_' + str(x[j]))) % D\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' Get probability estimation on x\n",
    "            INPUT:\n",
    "                x: features\n",
    "            OUTPUT:\n",
    "                probability of p(y = 1 | x; w)\n",
    "        '''\n",
    "\n",
    "        # parameters\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = {}\n",
    "\n",
    "        # wTx is the inner product of w and x\n",
    "        wTx = 0.\n",
    "        for i in self._indices(x):\n",
    "            sign = -1. if z[i] < 0 else 1.  # get sign of z[i]\n",
    "\n",
    "            # build w on the fly using z and n, hence the name - lazy weights\n",
    "            # we are doing this at prediction instead of update time is because\n",
    "            # this allows us for not storing the complete w\n",
    "            if sign * z[i] <= L1:\n",
    "                # w[i] vanishes due to L1 regularization\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                # apply prediction time L1, L2 regularization to z and get w\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "\n",
    "            wTx += w[i]\n",
    "\n",
    "        # cache the current w for update stage\n",
    "        self.w = w\n",
    "\n",
    "        # bounded sigmoid function, this is the probability estimation\n",
    "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
    "\n",
    "    def update(self, x, p, y):\n",
    "        ''' Update model using x, p, y\n",
    "            INPUT:\n",
    "                x: feature, a list of indices\n",
    "                p: click probability prediction of our model\n",
    "                y: answer\n",
    "            MODIFIES:\n",
    "                self.n: increase by squared gradient\n",
    "                self.z: weights\n",
    "        '''\n",
    "\n",
    "        # parameter\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "\n",
    "        # gradient under logloss\n",
    "        g = p - y\n",
    "\n",
    "        # update z and n\n",
    "        for i in self._indices(x):\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g\n",
    "\n",
    "\n",
    "def logloss(p, y):\n",
    "    ''' FUNCTION: Bounded logloss\n",
    "        INPUT:\n",
    "            p: our prediction\n",
    "            y: real answer\n",
    "        OUTPUT:\n",
    "            logarithmic loss of p given y\n",
    "    '''\n",
    "\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "\n",
    "def data(path, D):\n",
    "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
    "                   and for simplicity, we one-hot-encode everything\n",
    "        INPUT:\n",
    "            path: path to training or testing file\n",
    "            D: the max index that we can hash to\n",
    "        YIELDS:\n",
    "            ID: id of the instance, mainly useless\n",
    "            x: a list of hashed and one-hot-encoded 'indices'\n",
    "               we only need the index since all values are either 0 or 1\n",
    "            y: y = 1 if we have a click, else we have y = 0\n",
    "    '''\n",
    "\n",
    "    for t, row in enumerate(DictReader(open(path), delimiter=',')):\n",
    "\n",
    "        try:\n",
    "            ID= row['ID']\n",
    "            del row['ID']\n",
    "        except:\n",
    "            ID = 0\n",
    "            pass\n",
    "\n",
    "        # process target.\n",
    "        y = 0.\n",
    "        target='Approved'\n",
    "        #row['I1'] = str(row['Monthly_Income']) + str(row['Var5'])\n",
    "        row['I2'] = str(row['Monthly_Income']) + str(row['Existing_EMI'])\n",
    "        row['I3'] = str(row['Var1']) + str(row['Existing_EMI'])\n",
    "        row['I4'] = str(row['Var1']) + str(row['noofDays'])\n",
    "        #row['I5'] = str(row['Var1']) + str(row['Loan_Amount_Submitted'])\n",
    "        row['I6'] = str(row['Interest_Rate']) + str(row['dob_year'])\n",
    "        row['I7'] = str(row['dob_weekofyear']) + str(row['dob_day'])\n",
    "        #row['I7'] = str(row['Loan_Amount']) + str(row['Processing_Fee'])\n",
    "        #row['I8'] = str(row['Var5']) + str(row['Var4'])\n",
    "        row['I9'] = str(row['dob_month']) + str(row['dob_dayofweek'])\n",
    "        #lcd_weekofyear\n",
    "\n",
    "\n",
    "        if target in row:\n",
    "            if row[target] == '1':\n",
    "                y = 1.\n",
    "            del row[target]\n",
    "\n",
    "        # extract date\n",
    "\n",
    "        # turn hour really into hour, it was originally YYMMDDHH\n",
    "\n",
    "\n",
    "        # build x\n",
    "        x = []\n",
    "        for key in row:\n",
    "            value = row[key]\n",
    "\n",
    "            # one-hot encode everything with hash trick\n",
    "            index = abs(hash(key + '_' + value)) % D\n",
    "            x.append(index)\n",
    "\n",
    "        yield t, ID, x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 2018-01-21 00:58:21.015007\n",
      "creating submission file\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# start training #############################################################\n",
    "##############################################################################\n",
    "\n",
    "start = datetime.now()\n",
    "print(\"started at: %s\" % datetime.now())\n",
    "\n",
    "# initialize ourselves a learner\n",
    "learner = ftrl_proximal(alpha, beta, L1, L2, D, interaction)\n",
    "\n",
    "# start training\n",
    "for e in range(epoch):\n",
    "    loss = 0.\n",
    "    count = 0\n",
    "    for t, ID, x, y in data(train, D):  # data is a generator\n",
    "\n",
    "        p = learner.predict(x)\n",
    "\n",
    "        # if (holdout and t % holdout == 0):\n",
    "        # #     # Estimate progressive validation loss\n",
    "        #     loss += logloss(p, y)\n",
    "        #     count += 1\n",
    "        # else:\n",
    "        # #     # Use other samples to train the model\n",
    "        #     learner.update(x, p, y)\n",
    "\n",
    "        learner.update(x, p, y)\n",
    "        # if t % 1000000 == 0:\n",
    "        #     continue\n",
    "\n",
    "    #print('epoch: %s\\tval. logloss: %0.5f\\telapsed time: %s' % (e + 1, loss/count, str(datetime.now() - start)))\n",
    "\n",
    "#import pickle\n",
    "#pickle.dump(learner,open('ftrl3.p','w'))\n",
    "\n",
    "##############################################################################\n",
    "# start testing, and build Kaggle's submission file ##########################\n",
    "##############################################################################\n",
    "print ('creating submission file')\n",
    "with open(submission, 'w') as outfile:\n",
    "    outfile.write('ID,Approved\\n')\n",
    "    for t, ID, x, y in data(test, D):\n",
    "        p = learner.predict(x)\n",
    "        outfile.write('%s,%s\\n' % (ID, str(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-83d8acbf0cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATAPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ftrl_final.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ftrl_final2.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "preds = pd.read_csv(os.path.join(DATAPATH,\"results\",\"ftrl_final.csv\"))\n",
    "preds['ID'] = test['ID']\n",
    "preds.to_csv(\"ftrl_final2.csv\", index  = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
