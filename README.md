# Mckinsey Analytics Hackathon
This repository contains the code for McKinsey Hackathon by Analytics Vidhya for Sales Excellence

McKinsey Analytics Hackathon is conducted by Analytics Vidhya , it is India's largest Data Science portal i.e. for learning, Hackathons, blogs etc.

The Hackathon is to predict the sales Excellence for Mckinsey Analytics for it clients.

### Problem Statement
A digital arm of a bank faces challenges with lead conversions. The primary objective of this division is to increase customer acquisition through digital channels. The division was set up a few years back and the primary focus of the division over these years has been to increase the number of leads getting into the conversion funnel.

They source leads through various channels like search, display, email campaigns and via affiliate partners. As expected, they see differential conversion depending on the sources and the quality of these leads.

They now want to identify the leads' segments having a higher conversion ratio (lead to buying a product) so that they can specifically target these potential customers through additional channels and re-marketing. They have provided a partial data set for salaried customers from the last 3 months. They also capture basic details about customers. We need to identify the segment of customers with a high probability of conversion in the next 30 days.

### Evaluation Criteria
The Evaluation Criteria for this problem is AUC_ROC


More about the competition, duration, leaderboard etc. details
[click here](https://datahack.analyticsvidhya.com/contest/mckinsey-analytics-online-hackathon-ii/)

## Submission details

I have submitted lot of ensemble models like 4 models of XGBoost, 2 models of Random Forest and also 3 models of FTRL.
But my final model is of only **4 models of XGBoost**, which gave me [**18<sup>th</sup> place**](https://datahack.analyticsvidhya.com/contest/mckinsey-analytics-online-hackathon-ii/pvt_lb) in the public leaderboard**.

The main files are :-
  1. xgb1.py
  2. xgb2.py
  3. xgb3.py
  4. xgb4.py
  5. xgboost_preprocessing.py
  6. ensemble_xgb_all.py
  
 I also updated my other models which I tried, as someone can learn or improve it.
 This is my 1<sup>st</sup> competition and I dont want to build a huge ensemble model with 10-15 models.
